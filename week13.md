In this week, I came aross an interesting paper from James Wexler et. al., titled "The What-If Tool: Interactive Probing of Machine Learning Models". (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8807255)

In developing and deploying ML system, undertstanding the performance of ML model is significant and a quite challenging task. The authors created a tool called "What-If", an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding.

The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics.

I think it can be an extremely useful tool in many cases especially for those who lack of sophisticaed coding skills.
